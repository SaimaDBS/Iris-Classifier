# -*- coding: utf-8 -*-
"""CNN Fruit Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L44LIaBSgJOFninPwGUxpGxWgqmNxgoy
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import pandas as pd
# %matplotlib inline

tf.keras.backend.clear_session()
df = pd.read_csv('IRIS.csv')

df.head()

one_hot_df = pd.get_dummies(df['species'], prefix='species')
# print(one_hot_df)

X = df.drop(['species'],axis=1).values
y = one_hot_df.values

X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8)
X_train.shape, X_test.shape, y_train.shape,y_test.shape

model = tf.keras.models.Sequential(name='MLP')

n_hidden = 10
n_input = len(X[0])
n_output = len(y[0])

model.add((tf.keras.layers.Dense(n_hidden,input_dim=n_input, activation = 'relu',name='hidden')))
model.add(tf.keras.layers.Dense(n_output, activation = 'softmax',name='output'))

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(X_train, y_train, epochs = 100, validation_split=0.2, batch_size = 16, verbose=0)

fig, ax = plt.subplots(1,2,figsize = (16,4))
ax[0].plot(history.history['loss'],color='#EFAEA4',label = 'Training Loss')
ax[0].plot(history.history['val_loss'],color='#B2D7D0',label = 'Validation Loss')
ax[1].plot(history.history['accuracy'],color='#EFAEA4',label = 'Training Accuracy')
ax[1].plot(history.history['val_accuracy'],color='#B2D7D0',label = 'Validation Accuracy')
ax[0].legend()
ax[1].legend()
ax[0].set_xlabel('Epochs')
ax[1].set_xlabel('Epochs');
ax[0].set_ylabel('Loss')
ax[1].set_ylabel('Accuracy %');
fig.suptitle('MLP Training', fontsize = 24)

train_accuracy = model.evaluate(X_train,y_train)[1]
test_accuracy = model.evaluate(X_test,y_test)[1]
print(f'The training set accuracy for the model is {train_accuracy}\
    \n The test set accuracy for the model is {test_accuracy}')

history = model.fit(X_train, y_train, epochs = 50, validation_split=0.2, batch_size = 16, verbose=0)

fig, ax = plt.subplots(1,2,figsize = (16,4))
ax[0].plot(history.history['loss'],color='#EFAEA4',label = 'Training Loss')
ax[0].plot(history.history['val_loss'],color='#B2D7D0',label = 'Validation Loss')
ax[1].plot(history.history['accuracy'],color='#EFAEA4',label = 'Training Accuracy')
ax[1].plot(history.history['val_accuracy'],color='#B2D7D0',label = 'Validation Accuracy')
ax[0].legend()
ax[1].legend()
ax[0].set_xlabel('Epochs')
ax[1].set_xlabel('Epochs');
ax[0].set_ylabel('Loss')
ax[1].set_ylabel('Accuracy %');
fig.suptitle('MLP Training', fontsize = 24)

train_accuracy = model.evaluate(X_train,y_train)[1]
test_accuracy = model.evaluate(X_test,y_test)[1]
print(f'The training set accuracy for the model is {train_accuracy}\
    \n The test set accuracy for the model is {test_accuracy}')
